{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740134551190
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1740134567528
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Processed 30771 therapist-patient exchanges and saved to cbt_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import TrainerCallback, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def parse_conversation(content):\n",
        "    \"\"\"Extracts only Therapist-Patient conversation turns and ensures correct order.\"\"\"\n",
        "    # Extract only lines that start with \"Therapist:\" or \"Patient:\"\n",
        "    turns = re.findall(r'(?:Therapist|Patient):.*', content)\n",
        "    prompt_response_pairs = []\n",
        "\n",
        "    for i in range(len(turns) - 1):\n",
        "        speaker_1, text_1 = turns[i].split(\":\", 1)\n",
        "        speaker_2, text_2 = turns[i + 1].split(\":\", 1)\n",
        "\n",
        "        # Strip whitespace\n",
        "        text_1, text_2 = text_1.strip(), text_2.strip()\n",
        "\n",
        "        # Ensure we have an alternating Patient → Therapist order\n",
        "        if speaker_1 == \"Patient\" and speaker_2 == \"Therapist\":\n",
        "            prompt_response_pairs.append({\n",
        "                'instruction': \"You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:\",\n",
        "                'input': f\"{text_1}\\n\",\n",
        "                'output': f\"<think>Step by step, analyze the Patient's thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\\n{text_2}\"\n",
        "            })\n",
        "\n",
        "    return prompt_response_pairs\n",
        "\n",
        "def process_combined_file(file_path):\n",
        "    \"\"\"Reads and processes the input conversation file.\"\"\"\n",
        "    all_pairs = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Split content into individual conversations\n",
        "    conversations = content.split('</conversation>')\n",
        "\n",
        "    for conv in conversations:\n",
        "        if conv.strip():\n",
        "            conv_content = conv.strip()\n",
        "            pairs = parse_conversation(conv_content)\n",
        "            all_pairs.extend(pairs)\n",
        "\n",
        "    return all_pairs\n",
        "\n",
        "# Process the input file and create the dataset\n",
        "input_file = 'Synthetic_Transcripts.txt'\n",
        "output_file = 'cbt_dataset.json'\n",
        "\n",
        "prompt_response_pairs = process_combined_file(input_file)\n",
        "\n",
        "dataset_dict = {\n",
        "    'instruction': [pair['instruction'] for pair in prompt_response_pairs],\n",
        "    'input': [pair['input'] for pair in prompt_response_pairs],\n",
        "    'output': [pair['output'] for pair in prompt_response_pairs]\n",
        "}\n",
        "\n",
        "# Save the dataset dictionary as a JSON file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    json.dump(dataset_dict, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Processed {len(prompt_response_pairs)} therapist-patient exchanges and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740134567592
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔹 **Sample from CBT Dataset** 🔹\n",
            "\n",
            "🔢 Sample Index: 2430\n",
            "📝 Instruction:\n",
            "You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:\n",
            "\n",
            "🗣️ Input:\n",
            "Photography has been a nice outlet for me. I went on a hike last weekend and took some photos of the landscape. It felt good to focus on something I enjoy and be out in nature.\n",
            "\n",
            "\n",
            "💡 Output:\n",
            "<think>Step by step, analyze the Patient's thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\n",
            "That's wonderful. Engaging in activities that bring you joy and help you connect with yourself and your environment is an essential part of managing stress and maintaining your mental well-being. Let's make sure to continue incorporating those activities into your routine.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "dataset_file = 'cbt_dataset.json'\n",
        "\n",
        "with open(dataset_file, 'r', encoding='utf-8') as file:\n",
        "    dataset = json.load(file)\n",
        "\n",
        "# Get total number of examples\n",
        "num_samples = len(dataset[\"instruction\"])\n",
        "\n",
        "# Select a random index\n",
        "random_index = random.randint(0, num_samples - 1)\n",
        "\n",
        "# Display one example\n",
        "print(\"\\n🔹 **Sample from CBT Dataset** 🔹\\n\")\n",
        "print(f\"🔢 Sample Index: {random_index}\")\n",
        "print(f\"📝 Instruction:\\n{dataset['instruction'][random_index]}\\n\")\n",
        "print(f\"🗣️ Input:\\n{dataset['input'][random_index]}\\n\")\n",
        "print(f\"💡 Output:\\n{dataset['output'][random_index]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1740134571487
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully merged datasets! New dataset saved as cbt_mentalchat_dataset.json\n",
            "🔹 Total Samples in Combined Dataset: 46855\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load existing dataset\n",
        "cbt_dataset_file = 'cbt_dataset.json'\n",
        "\n",
        "with open(cbt_dataset_file, 'r', encoding='utf-8') as file:\n",
        "    cbt_dataset = json.load(file)\n",
        "\n",
        "# Load ShenLab/MentalChat16K from Hugging Face\n",
        "mentalchat_dataset = load_dataset(\"ShenLab/MentalChat16K\", split=\"train\")\n",
        "\n",
        "# Extract relevant fields from MentalChat16K\n",
        "mentalchat_pairs = []\n",
        "\n",
        "for sample in mentalchat_dataset:\n",
        "    mentalchat_pairs.append({\n",
        "        \"instruction\": \"You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:\",\n",
        "        \"input\": sample[\"input\"],  # Keeping structure consistent\n",
        "        \"output\": f\"<think>Step by step, analyze the Patient's thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\\n{sample['output']}\"  # Correctly formatted\n",
        "    })\n",
        "\n",
        "# Merge both datasets\n",
        "combined_dataset = cbt_dataset  # Start with existing dataset\n",
        "combined_dataset[\"instruction\"].extend(pair[\"instruction\"] for pair in mentalchat_pairs)\n",
        "combined_dataset[\"input\"].extend(pair[\"input\"] for pair in mentalchat_pairs)\n",
        "combined_dataset[\"output\"].extend(pair[\"output\"] for pair in mentalchat_pairs)\n",
        "\n",
        "# Save merged dataset\n",
        "output_file = \"cbt_mentalchat_dataset.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(combined_dataset, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✅ Successfully merged datasets! New dataset saved as {output_file}\")\n",
        "print(f\"🔹 Total Samples in Combined Dataset: {len(combined_dataset['instruction'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1740134601643
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Qwen2 patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.151 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Dataset size: 46855\n",
            "Dataset features: {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n",
            "First shuffled example: {'instruction': 'You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:', 'input': \"Hi, I'm back for session 10. Here is the summary you provided me at the end of our last session:\\n\", 'output': \"<think>Step by step, analyze the Patient's thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\\nHello Emilio, it's great to see you again for our 10th session together. Before we dive in, I want to acknowledge my limitations as an AI therapist. While I can provide support and guidance, I don't have access to real-time information or personal details beyond what you share with me. There's also a potential for me to generate inaccurate information at times. Additionally, I'm not equipped to provide crisis intervention or emergency services, and I can't directly observe or interpret non-verbal cues. With that said, I'm here to support you in the best way I can. How have you been since our last session?\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43fd8bd87a754f9eace523ab2e11dbd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/46855 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset successfully shuffled and split!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define model and tokenizer parameters\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\"\n",
        "max_seq_length = 4069\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    cache_dir=\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/a10048/code/models\"\n",
        ")\n",
        "\n",
        "# Configure the PEFT model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Define the EOS_TOKEN and Alpaca prompt format\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Function to format prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input_text, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(combined_dataset)\n",
        "\n",
        "# Shuffle dataset before splitting\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"Dataset features:\", dataset.features)\n",
        "print(\"First shuffled example:\", dataset[0])\n",
        "\n",
        "# Format the entire dataset\n",
        "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Split the formatted dataset into train, validation, and test sets\n",
        "train_val_test = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_val = train_val_test['train'].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "formatted_train_dataset = train_val['train']\n",
        "formatted_val_dataset = train_val['test']\n",
        "formatted_test_dataset = train_val_test['test']\n",
        "\n",
        "print(\"Dataset successfully shuffled and split!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740135711313
        }
      },
      "outputs": [],
      "source": [
        "# Define a callback for logging losses\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_file):\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero and logs is not None:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                if 'loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Loss: {logs['loss']}\\n\")\n",
        "                if 'eval_loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\\n\")\n",
        "\n",
        "log_file = os.path.join(\"outputs\", \"loss_log.txt\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train_dataset,\n",
        "    eval_dataset=formatted_val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=32,  \n",
        "        per_device_eval_batch_size=32,   \n",
        "        gradient_accumulation_steps=8,  \n",
        "        warmup_steps=int(0.05 * (len(formatted_train_dataset) // (32 * 8))), \n",
        "        num_train_epochs=2,\n",
        "        gradient_checkpointing=True, \n",
        "        learning_rate=5e-5,  \n",
        "        bf16=True,  \n",
        "        logging_steps=10, \n",
        "        optim=\"adamw_torch_fused\",  \n",
        "        weight_decay=0.005,  \n",
        "        lr_scheduler_type=\"cosine\",  \n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50, \n",
        "    ),\n",
        "    callbacks=[LossLoggingCallback(log_file)],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained_gguf(\"DeepLlama\", tokenizer, quantization_method=\"q8_0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740120346621
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "Processed 31695 prompt-response pairs and saved to cbt_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "from xml.etree import ElementTree as ET\n",
        "from unsloth import FastLanguageModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import TrainerCallback, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def parse_conversation(content):\n",
        "    # Split the content into turns\n",
        "    turns = re.split(r'\\n(?=(?:Therapist:|Patient:))', content)\n",
        "    conversation_history = []\n",
        "    prompt_response_pairs = []\n",
        "\n",
        "    for turn in turns:\n",
        "        speaker, _, text = turn.partition(':')\n",
        "        text = text.strip()\n",
        "\n",
        "        # Collect each turn into conversation history\n",
        "        conversation_history.append(f\"{speaker}: {text}\")\n",
        "\n",
        "        # If the turn is from the AI, generate a prompt-response pair\n",
        "        if speaker == 'Therapist':\n",
        "            # Use the entire conversation history up to this point as the prompt\n",
        "            prompt = '\\n'.join(conversation_history[:-1])\n",
        "            # Add the prompt-response pair to the list with <think> structure\n",
        "            prompt_response_pairs.append({\n",
        "                'instruction': \"You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:\",\n",
        "                'input': prompt,\n",
        "                'output': f\"<think>Step by step, analyze the client’s thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\\n{text}\"\n",
        "            })\n",
        "\n",
        "    return prompt_response_pairs\n",
        "\n",
        "def process_combined_file(file_path):\n",
        "    all_pairs = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Split the content into individual conversations\n",
        "    conversations = content.split('</conversation>')\n",
        "\n",
        "    for conv in conversations:\n",
        "        if conv.strip():\n",
        "            conv_content = conv.split('\\n', 1)[1].strip() if '\\n' in conv else conv.strip()\n",
        "            pairs = parse_conversation(conv_content)\n",
        "            all_pairs.extend(pairs)\n",
        "\n",
        "    return all_pairs\n",
        "\n",
        "\n",
        "# Process the input file and create the dataset\n",
        "input_file = 'Synthetic_Transcripts.txt'\n",
        "output_file = 'cbt_dataset.json'\n",
        "prompt_response_pairs = process_combined_file(input_file)\n",
        "dataset_dict = {\n",
        "    'instruction': [pair['instruction'] for pair in prompt_response_pairs],\n",
        "    'input': [pair['input'] for pair in prompt_response_pairs],\n",
        "    'output': [pair['output'] for pair in prompt_response_pairs]\n",
        "}\n",
        "\n",
        "# Save the dataset dictionary as a JSON file\n",
        "with open(output_file, 'w', encoding='utf-8') as file:\n",
        "    json.dump(dataset_dict, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Processed {len(prompt_response_pairs)} prompt-response pairs and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1740120358823
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.151 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Dataset size: 31695\n",
            "Dataset features: {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n",
            "First example: {'instruction': 'You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist:', 'input': 'Patient: Hi, I\\'m back for session 12. Here is the summary you provided me at the end of our last session:\\n\\n- Main topics discussed: Challenging perfectionistic thoughts, addressing guilt around self-care, and improving sleep quality.\\n- Techniques or exercises used: Thought records, CBT triangle, downward arrow technique, identifying intermediate and core beliefs, cognitive restructuring, and sleep hygiene strategies. In future sessions, we will continue practicing these techniques and introduce new ones as needed, such as cognitive and behavioral experiments, reframing core beliefs, and addressing specific depressive symptoms like rumination and social withdrawal.\\n- Homework assigned: Continue working on thought records, focusing on challenging perfectionistic thoughts and guilt around self-care; implement sleep hygiene strategies, including establishing a consistent sleep routine, engaging in relaxing activities before bed, and creating a comfortable sleep environment; gather evidence against the core belief that taking time for oneself means being a failure or unworthy of love and respect.\\n- Patient\\'s progress, challenges, depression symptoms, and important personal details: Amira demonstrated progress in identifying and challenging perfectionistic thoughts and is open to practicing self-compassion and sleep hygiene techniques. She continues to struggle with low energy, motivation, and self-critical thoughts but is committed to engaging in the therapeutic process and making meaningful changes.\\n- Core beliefs and intermediate beliefs: Reviewed Amira\\'s core belief about being a failure and unworthy of love and respect if she takes time for herself, and explored its connection to intermediate beliefs about always needing to be productive and viewing self-care as selfish or indulgent. Introduced techniques for challenging these beliefs and developing more balanced and self-compassionate perspectives.\\n- Important information to carry forward to future sessions: Amira\\'s tendency to be self-critical and hold herself to unrealistic standards; her commitment to practicing self-compassion, sleep hygiene, and engaging in enjoyable activities; the specific, measurable goals identified for therapy; the progress made in challenging negative beliefs and developing more balanced perspectives.\\n- Plan for the next session: Review Amira\\'s progress with homework assignments, particularly the thought records, sleep hygiene, and gathering evidence against her core belief; continue practicing cognitive restructuring, mindfulness, and reframing core beliefs; explore any challenges or insights that came up during the week; introduce new techniques as needed to address specific depressive symptoms and expand on cognitive and behavioral experiments.\\nTherapist: Hello Amira, it\\'s great to see you again for session 12. Thank you for sharing the summary from our last session. Before we dive in, I want to acknowledge my limitations as an AI therapist. While I can provide support and guidance based on the information you share with me, I don\\'t have access to real-time information or personal details beyond what you provide in our sessions. Additionally, there\\'s a potential for me to generate inaccurate information, and I\\'m unable to provide crisis intervention or emergency services. I also lack the ability to directly observe or interpret non-verbal cues. With that said, I\\'m committed to working with you to the best of my abilities. How are you feeling today?\\nPatient: Thank you for the reminder about your limitations. I understand and appreciate your commitment to helping me. I\\'ve been feeling a bit better since our last session, but I\\'m still struggling with low energy and motivation. I\\'ve been trying to implement the sleep hygiene strategies we discussed, and I think they\\'re starting to help a little.\\nTherapist: I\\'m glad to hear that you\\'ve noticed some improvement with your sleep, even if it\\'s just a little. Small changes can make a big difference over time. Let\\'s start by exploring your mood and experiences since our last session. Can you tell me more about how your energy and motivation have been affected?\\nPatient: Well, I\\'ve been trying to push myself to do more, but I still feel exhausted most of the time. Even small tasks seem to drain me, and I find myself putting things off because I don\\'t have the energy. It\\'s frustrating because I know there are things I want to do, but I just can\\'t seem to get started.\\nTherapist: It sounds like you\\'re experiencing a lot of frustration and exhaustion. That can be a challenging cycle to break. Have you noticed any patterns or specific situations that seem to trigger these feelings?\\nPatient: I think it\\'s worst in the mornings, especially on weekdays when I know I have to go to work. The thought of facing the day just feels overwhelming. I\\'ve also noticed that I tend to feel more drained after social interactions, even if they\\'re positive.\\nTherapist: That\\'s a great observation about the patterns you\\'ve noticed. Recognizing those triggers is an important step in finding ways to manage them. Let\\'s keep that in mind as we plan our agenda for today\\'s session. What would you like to prioritize?\\nPatient: I\\'d like to focus on finding ways to improve my energy and motivation. I think if I could make some progress there, it would help with a lot of the other challenges I\\'m facing.\\nTherapist: Absolutely, that\\'s a great goal. We can definitely explore some strategies for improving energy and motivation. I\\'d also like to check in on your progress with the thought records and challenging perfectionistic thoughts. And if we have time, we could discuss some cognitive and behavioral experiments to help gather evidence against your core belief about self-care. How does that sound?\\nPatient: That sounds good. I\\'ve been working on the thought records, but I\\'m still finding it challenging to reframe my thoughts in the moment. I think some more practice with that would be helpful.\\nTherapist: Great, we can definitely review your thought records and practice reframing together. Before we dive into that, let\\'s set some specific goals for our session today. What would you like to achieve by the end of our time together?\\nPatient: By the end of the session, I\\'d like to have a better understanding of how my thoughts are impacting my energy and motivation, and some concrete strategies I can use to start making changes in those areas. I\\'d also like to feel more confident in my ability to challenge my perfectionistic thoughts and beliefs about self-care.\\nPatient: I\\'ve been trying to work on the thought records, but I\\'m still finding it hard to reframe my thoughts in the moment. I think I could use some more practice with that.\\nTherapist: I\\'m glad you brought that up. Reframing thoughts can be challenging, especially when we\\'re dealing with deeply ingrained patterns of thinking. Let\\'s take a look at one of your thought records from the past week. Can you share an example of a situation where you struggled to reframe your thoughts?\\nPatient: Sure. Last Wednesday, I was running late for a meeting at work, and I couldn\\'t find the report I needed. I started thinking, \"I\\'m such a failure. I can\\'t even keep track of my own work. Everyone\\'s going to think I\\'m incompetent.\"', 'output': \"<think>Step by step, analyze the client’s thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\\nThat's a great example. Let's break this down using the thought record. First, can you identify the emotions you were feeling in that moment and rate their intensity on a scale of 0-100?\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a9eeda0f30f499c92f584ac3df3b2d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/31695 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define model and tokenizer parameters\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "max_seq_length = 10000\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Configure the PEFT model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Define the EOS_TOKEN and Alpaca prompt format\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Function to format prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"Dataset features:\", dataset.features)\n",
        "print(\"First example:\", dataset[30])\n",
        "\n",
        "# Format the entire dataset\n",
        "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Split the formatted dataset into train, validation, and test sets\n",
        "train_val_test = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_val = train_val_test['train'].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "formatted_train_dataset = train_val['train']\n",
        "formatted_val_dataset = train_val['test']\n",
        "formatted_test_dataset = train_val_test['test']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740121000228
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/jupyter_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/anaconda/envs/jupyter_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82c49623b8cd470f8fa73c9c4e6991d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=2):   0%|          | 0/25672 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3037b8ec57f34f58b7647b07c41236e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=2):   0%|          | 0/25672 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bf2af92fc304753a98241581008c3d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=2):   0%|          | 0/25672 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "576f16ac89784141a20e211be7798561",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to eval dataset (num_proc=2):   0%|          | 0/2853 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "869c94d974b5456da5abf155e1c47b55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset (num_proc=2):   0%|          | 0/2853 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b1579c6d8b241f192ed773d56b99792",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset (num_proc=2):   0%|          | 0/2853 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 25,672 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 16\n",
            "\\        /    Total batch size = 512 | Total steps = 50\n",
            " \"-____-\"     Number of trainable parameters = 20,971,520\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibrahimshaban1994\u001b[0m (\u001b[33mibrahimshaban1994-university-of-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/a10048/code/Users/ibrahim.shabanqabaqebo/wandb/run-20250221_065022-dle4k5yo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface/runs/dle4k5yo' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface' target=\"_blank\">https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface/runs/dle4k5yo' target=\"_blank\">https://wandb.ai/ibrahimshaban1994-university-of-london/huggingface/runs/dle4k5yo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 51\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[LossLoggingCallback(log_file)],\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained_gguf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepLlama\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, quantization_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq8_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:334\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define a callback for logging losses\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_file):\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero and logs is not None:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                if 'loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Loss: {logs['loss']}\\n\")\n",
        "                if 'eval_loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\\n\")\n",
        "\n",
        "log_file = os.path.join(\"outputs\", \"loss_log.txt\")\n",
        "\n",
        "# Configure the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train_dataset,\n",
        "    eval_dataset=formatted_val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=16,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=1,\n",
        "        gradient_checkpointing=True,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.1,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=10,\n",
        "    ),\n",
        "    callbacks=[LossLoggingCallback(log_file)],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained_gguf(\"DeepLlama\", tokenizer, quantization_method=\"q8_0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740090279080
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Path to the JSON file with your augmented dataset.\n",
        "json_path = \"expanded_dataset.json\"\n",
        "\n",
        "# Load the JSON file.\n",
        "print(\"Loading dataset from JSON...\")\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Create a Hugging Face dataset from the JSON data.\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"Dataset features:\", dataset.features)\n",
        "print(\"First example:\", dataset[0])\n",
        "\n",
        "# Split the dataset into train, validation, and test sets.\n",
        "print(\"\\nSplitting dataset into train/validation/test sets...\")\n",
        "train_val_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_val = train_val_test[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "formatted_train_dataset = train_val[\"train\"]\n",
        "formatted_val_dataset = train_val[\"test\"]\n",
        "formatted_test_dataset = train_val_test[\"test\"]\n",
        "\n",
        "print(\"Train set size:\", len(formatted_train_dataset))\n",
        "print(\"Validation set size:\", len(formatted_val_dataset))\n",
        "print(\"Test set size:\", len(formatted_test_dataset))\n",
        "\n",
        "# Continue with further processing or training..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740090283397
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory Allocated: 0.00 MB\n",
            "GPU Memory Reserved: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024 / 1024  # in MB\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024 / 1024    # in MB\n",
        "    print(f\"GPU Memory Allocated: {allocated:.2f} MB\")\n",
        "    print(f\"GPU Memory Reserved: {reserved:.2f} MB\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739964712383
        }
      },
      "outputs": [],
      "source": [
        "# Define model and tokenizer parameters\n",
        "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\"\n",
        "max_seq_length = 10000\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Configure the PEFT model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Define the EOS_TOKEN and Alpaca prompt format\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Function to format prompts\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"Instruction\"]  \n",
        "    inputs = examples[\"Input\"]  \n",
        "    outputs = examples[\"Assistant\"]  \n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "print(\"Dataset features:\", dataset.features)\n",
        "print(\"First example:\", dataset[30])\n",
        "\n",
        "# Format the entire dataset\n",
        "formatted_dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Split the formatted dataset into train, validation, and test sets\n",
        "train_val_test = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_val = train_val_test['train'].train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "formatted_train_dataset = train_val['train']\n",
        "formatted_val_dataset = train_val['test']\n",
        "formatted_test_dataset = train_val_test['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1740087502183
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted Example: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are an AI CBT therapist. Respond appropriately in the following conversation, you must give only one response as a therapist, you never break your role as a CBT therapist:\n",
            "\n",
            "### Input:\n",
            "Patient: Hi, I'm back for session 7. Here is the summary you provided me at the end of our last session:\n",
            "\n",
            "- Main topics discussed: Managing financial stress, challenging negative thoughts related to self-worth and success, addressing feelings of guilt around relying on friends for support\n",
            "- Techniques or exercises used: \n",
            "  - Behavioral activation: Engaging in planned activities like hiking and reading before bed to counteract avoidance and withdrawal. Plan to continue implementing this technique and monitor mood in response to activities.\n",
            "  - Thought records: Challenging negative thoughts related to self-worth, success, and financial stress. Plan to continue using thought records to build skill in reframing thoughts.\n",
            "  - Mindfulness and grounding techniques: Using mindfulness to manage overwhelming emotions and grounding exercises to cope with financial stress and guilt. Plan to continue practicing these techniques, particularly in moments of distress.\n",
            "  - Communication skills: Encouraged patient to have an open and honest conversation with a trusted friend about feelings of guilt. Plan to follow up on this in the next session.\n",
            "- Homework assigned: \n",
            "  - Continue practicing mindfulness and grounding techniques, particularly when experiencing anxious thoughts about finances or guilt around relying on friends.\n",
            "  - Keep using thought records to challenge negative thoughts related to self-worth and success.\n",
            "  - Have an open and honest conversation with a trusted friend about feelings of guilt.\n",
            "- Patient's progress, challenges, depression symptoms and important personal details:\n",
            "  - Patient reported some improvements in mood but still struggles with overwhelming emotions and feelings of worthlessness, particularly related to financial stress and the loss of another client.\n",
            "  - Patient is continuing to practice techniques from previous sessions and is starting to notice some improvements in mood. They are still struggling with negative thoughts but are willing to engage in behavioral activation and thought challenging.\n",
            "  - Depression symptoms: Persistent feelings of sadness, worthlessness, and lack of motivation. Disrupted sleep patterns, with bouts of insomnia followed by periods of excessive sleeping. Struggles with work and maintaining professional boundaries.\n",
            "  - Personal details: The patient is a 31-year-old non-binary individual of East Asian descent, self-employed teaching dance and offering private yoga and meditation sessions. They have a small but tight-knit group of friends and a close relationship with their mother.\n",
            "- Core beliefs and intermediate beliefs: Not discussed in this session.\n",
            "- Important information to carry forward to future sessions: \n",
            "  - Continue monitoring patient's progress with behavioral activation and thought records.\n",
            "  - Follow up on patient's conversation with a trusted friend about feelings of guilt.\n",
            "  - Explore any new challenges or stressors that arise in the coming week.\n",
            "- Plan for the next session: \n",
            "  - Review homework and progress with mindfulness, grounding techniques, and thought records.\n",
            "  - Discuss patient's conversation with a trusted friend about feelings of guilt.\n",
            "  - Continue to work on challenging negative thoughts and building emotional resilience.\n",
            "  - Address any new challenges or concerns that arise in the coming week.\n",
            "Therapist: Hello Jamie, it's good to see you again. Thank you for sharing the summary from our last session. Before we dive in, I want to acknowledge my limitations as an AI therapist. While I can provide support and guidance, I don't have access to real-time information or personal details beyond what you share with me. There's also a potential for me to generate inaccurate information at times. Additionally, I'm not equipped to provide crisis intervention or emergency services, and I can't directly observe or interpret non-verbal cues. With that said, I'm here to support you in the best way I can. How have you been since our last session?\n",
            "Patient: Thanks for the reminder, Nova. I understand your limitations. Since our last session, I've been trying to practice the techniques we discussed, like mindfulness and thought records. It's been challenging, but I think I'm making some progress. I had that conversation with my friend about feeling guilty for relying on them, and it went better than I expected. They were really understanding and supportive.\n",
            "Therapist: I'm glad to hear that you've been practicing the techniques and that your conversation with your friend went well. It takes courage to open up like that. Can you tell me more about how you've been feeling emotionally and what challenges you've faced since our last session?\n",
            "Patient: Emotionally, I've had some ups and downs. I had a few good days where I felt more motivated and hopeful, but then I lost another client, and it sent me into a bit of a downward spiral. I started having those negative thoughts again, like \"I'm not good enough\" and \"I'll never be successful.\" It's been hard to shake off those feelings of worthlessness.\n",
            "Therapist: It's understandable to have those negative thoughts and feelings, especially after losing another client. I'm here to support you through this. Based on what you've shared, it seems like it would be helpful for us to focus on managing these negative thoughts and emotions today. What do you think about setting an agenda that includes reviewing your thought records, discussing the loss of your client and its impact on your mood, and exploring additional coping strategies for managing distressing emotions?\n",
            "Patient: That sounds good to me. I definitely need some help with managing these negative thoughts and feelings. I'm also wondering if we could talk about how to handle the financial stress that comes with losing clients. It's been weighing on me heavily.\n",
            "Therapist: Absolutely, we can certainly include discussing strategies for managing financial stress in our agenda. Here's what I propose for today's session:\n",
            "\n",
            "1. Review your thought records and discuss any challenges or insights you've had while using this technique.\n",
            "2. Explore the emotional impact of losing another client and identify coping strategies for managing the distressing thoughts and feelings that have arisen.\n",
            "3. Discuss strategies for managing financial stress and developing a plan to address your financial concerns.\n",
            "4. Introduce additional distress tolerance techniques to help you manage overwhelming emotions in the moment.\n",
            "\n",
            "How does this agenda sound to you? Is there anything else you'd like to add or focus on today?\n",
            "Patient: The agenda sounds good to me. I'm ready to dive in.\n",
            "Therapist: Great! Let's start by reviewing your thought records. Can you share some examples of negative thoughts you've had since our last session and how you've used the thought records to challenge them?\n",
            "Patient: Sure. One thought that kept coming up was \"I'm not good enough.\" I lost another client, and it made me feel like I'm just not cut out for this work. When I used the thought record, I tried to look for evidence against that thought. I reminded myself of the positive feedback I've received from other clients and the fact that I've been able to maintain my business for this long. But it's still hard to shake that feeling of not being good enough.\n",
            "Therapist: It's great that you were able to identify evidence against the negative thought. Challenging these thoughts takes practice, and it's normal to still feel some emotional attachment to them even when we logically know they're not entirely true. Did you notice any shift in the intensity of your emotions after completing the thought record?\n",
            "Patient: Yeah, I did feel a little better after going through the process. It helped me see things from a different perspective. But the relief was temporary. The negative thoughts keep coming back.\n",
            "Therapist: That's a common experience. Our negative thoughts can be persistent, especially when we're dealing with stressful situations like losing a client. The goal of using thought records is not to eliminate negative thoughts completely but to gradually build our ability to challenge them and reduce their impact on our emotions and behaviors. With continued practice, it will become easier to reframe these thoughts and find a more balanced perspective.\n",
            "Patient: That makes sense. I guess I just need to keep at it.\n",
            "Therapist: Exactly. Consistency is key. And remember, it's not about being perfect or never having negative thoughts. It's about developing the skill to manage them in a healthier way. Let's talk more about the emotional impact of losing another client. When you found out about the loss, what emotions did you experience, and how did you cope with them in the moment?\n",
            "Patient: When I found out about losing the client, I felt really anxious and hopeless. I started worrying about how I would pay my bills and what this meant for my business. I tried to do some deep breathing, but it was hard to calm down.\n",
            "Therapist: It's understandable to feel anxious and overwhelmed in that situation. Deep breathing can be helpful, but sometimes we need additional strategies to manage intense emotions. I'd like to introduce you to a technique called \"grounding\" which can be useful when you're feeling overwhelmed or consumed by negative thoughts.\n",
            "\n",
            "The idea behind grounding is to shift your focus away from your thoughts and onto your physical senses or surroundings. One simple grounding exercise is called the \"5-4-3-2-1\" technique. Here's how it works:\n",
            "\n",
            "1. Identify 5 things you can see around you. Take a moment to notice the details of each object.\n",
            "2. Identify 4 things you can touch. Notice the texture, temperature, and sensation of each item.\n",
            "3. Identify 3 things you can hear. Try to focus on the subtleties of each sound.\n",
            "4. Identify 2 things you can smell. Take a deep breath and try to identify any scents in your environment.\n",
            "5. Identify 1 thing you can taste. This could be as simple as noticing the taste in your mouth or taking a sip of a drink.\n",
            "\n",
            "The purpose of this exercise is to help you reconnect with the present moment and distract yourself from distressing thoughts. What do you think about trying this technique the next time you feel overwhelmed by negative thoughts or emotions?\n",
            "Patient: That sounds interesting. I can see how focusing on my senses could help distract me from my thoughts. I'll give it a try next time I'm feeling really anxious or down.\n",
            "Therapist: Great! I think grounding could be a helpful addition to your coping toolkit. Remember, the goal isn't to eliminate negative thoughts or emotions but to find ways to manage them so they don't consume you.\n",
            "\n",
            "Another strategy we can discuss is called \"behavioral activation.\" This involves intentionally engaging in activities that have the potential to boost your mood or give you a sense of accomplishment, even when you don't feel motivated to do them. The idea is that by engaging in these activities, you can start to break the cycle of avoidance and withdrawal that often accompanies depression.\n",
            "\n",
            "For example, you mentioned that you enjoy hiking but have been struggling to find the motivation to go. Setting a small, achievable goal related to hiking, such as going for a short walk on a nearby trail, could be a form of behavioral activation. What do you think about incorporating some planned activities into your week to help counteract the avoidance and withdrawal?\n",
            "Patient: I like the idea of setting small goals related to activities I enjoy. It's hard to motivate myself when I'm feeling down, but I can see how pushing myself to do something could help improve my mood. I guess I could start by planning a short hike this weekend and see how it goes.\n",
            "\n",
            "### Response:\n",
            "<think>Step by step, analyze the Patient's thoughts: identify patterns and distortions, evaluate supporting evidence, and restructure them into balanced, evidence-based alternatives for healthier thinking...</think>\n",
            "I'm glad you're open to trying behavioral activation. Setting small, achievable goals related to activities you enjoy can be a powerful way to break the cycle of avoidance and improve your mood. As we discussed, start with a short hike this weekend and pay attention to how it impacts your emotions and overall well-being.\n",
            "\n",
            "Another aspect of behavioral activation is monitoring your mood and energy levels throughout the day. This can help you identify patterns and make connections between your activities and your emotional state. I'd like to introduce you to a simple mood and energy tracking system that you can use to gather this information.\n",
            "\n",
            "The idea is to rate your mood and energy levels on a scale of 0-10 at regular intervals throughout the day, such as morning, afternoon, and evening. You can use a simple notebook or a mood tracking app on your phone. In addition to rating your mood and energy, jot down any notable activities, events, or thoughts that may have influenced your ratings.\n",
            "\n",
            "For example, you might note that your mood was a 6 and your energy was a 5 in the morning, and you had a productive work session. Then, in the afternoon, your mood dropped to a 3 and your energy to a 4 after receiving a stressful email from a client. Tracking this information over time can help you identify patterns and make informed decisions about how to structure your days to support your mental health.\n",
            "\n",
            "What do you think about incorporating mood and energy tracking into your daily routine?<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(\"Formatted Example:\", formatted_train_dataset[30][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1740090323335
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory Allocated: 5626.65 MB\n",
            "GPU Memory Reserved: 5628.00 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024 / 1024  # in MB\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024 / 1024    # in MB\n",
        "    print(f\"GPU Memory Allocated: {allocated:.2f} MB\")\n",
        "    print(f\"GPU Memory Reserved: {reserved:.2f} MB\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740090967499
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import TrainerCallback, TrainingArguments\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define a callback for logging losses\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_file):\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.is_local_process_zero and logs is not None:\n",
        "            with open(self.log_file, 'a') as f:\n",
        "                if 'loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Loss: {logs['loss']}\\n\")\n",
        "                if 'eval_loss' in logs:\n",
        "                    f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\\n\")\n",
        "\n",
        "log_file = os.path.join(\"outputs\", \"loss_log.txt\")\n",
        "\n",
        "\n",
        "# Configure the SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train_dataset,\n",
        "    eval_dataset=formatted_val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=16,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=1,\n",
        "        gradient_checkpointing=True,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "    ),\n",
        "    callbacks=[LossLoggingCallback(log_file)],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained_gguf(\"Fine Tuned deepmental 8b - new format\", tokenizer, quantization_method=\"q8_0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
