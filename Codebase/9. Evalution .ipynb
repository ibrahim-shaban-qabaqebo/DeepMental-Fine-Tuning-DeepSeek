{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740674202195
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from bert_score import score as bert_score\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1740674250108
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8548f94abb864ec6beb41831561dd17c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6e8a3912a38420f82a91d7664434192",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b33eebe013a34dcb908484bd9dd51cad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3db9091b78d54940aecf453270ebf4c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac018155eb084fb6919c0d6184ce6dce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e98e55697ca74a84b8e0f28f11411674",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "237654b483114148a176d7ae13ccfbcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46b87dcf0eaa47c69eb8b3ef5c3aa3f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Hugging Face metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "\n",
        "# Load the dataset with responses and expected outputs\n",
        "df = pd.read_csv(\"final_combined_with_expected_responses.csv\")\n",
        "\n",
        "# Ensure responses are not empty or invalid\n",
        "df = df[df[\"output\"].notna()]\n",
        "\n",
        "# Load model and tokenizer for perplexity calculation\n",
        "perplexity_model_name = \"EleutherAI/gpt-neo-1.3B\"  # Smaller model to save resources\n",
        "perplexity_tokenizer = AutoTokenizer.from_pretrained(perplexity_model_name)\n",
        "perplexity_model = AutoModelForCausalLM.from_pretrained(perplexity_model_name).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1740674253128
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to compute Perplexity\n",
        "def compute_perplexity(text):\n",
        "    encodings = perplexity_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = perplexity_model(**encodings, labels=encodings[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "    return torch.exp(loss).item()  # Perplexity is e^loss\n",
        "\n",
        "# Compute DISTINCT-1 & DISTINCT-2 (Measures response diversity)\n",
        "def compute_distinct_ngram(texts, n):\n",
        "    ngram_counts = Counter()\n",
        "    total_ngrams = 0\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "        ngram_counts.update(ngrams)\n",
        "        total_ngrams += len(ngrams)\n",
        "    \n",
        "    return len(ngram_counts) / total_ngrams if total_ngrams > 0 else 0\n",
        "\n",
        "# Compute BLEU score\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    ref_tokens = reference.split()\n",
        "    hyp_tokens = hypothesis.split()\n",
        "    return sentence_bleu([ref_tokens], hyp_tokens)\n",
        "\n",
        "# Define models to evaluate from the CSV file\n",
        "model_versions = [\n",
        "    (\"response_with_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_Traditional_responses\"),\n",
        "    (\"response_without_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_without_think\", \"generated_Fine_Tune_DeepMental_responses\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740674258796
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize evaluation storage\n",
        "evaluation_results = []\n",
        "\n",
        "# Run evaluation for each model\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Evaluating: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    \n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "\n",
        "    # Compute Perplexity\n",
        "    perplexities = [compute_perplexity(text) for text in predictions]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    bleu_scores = [compute_bleu(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "\n",
        "    # Compute BERTScore (Precision, Recall, F1)\n",
        "    P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
        "\n",
        "    # Compute DISTINCT scores\n",
        "    distinct_1 = compute_distinct_ngram(predictions, 1)\n",
        "    distinct_2 = compute_distinct_ngram(predictions, 2)\n",
        "\n",
        "    # Store results\n",
        "    evaluation_results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Response Type\": response_column,\n",
        "        \"Perplexity (Avg)\": np.mean(perplexities),\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
        "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
        "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"BERTScore Precision\": np.mean(P.tolist()),\n",
        "        \"BERTScore Recall\": np.mean(R.tolist()),\n",
        "        \"BERTScore F1\": np.mean(F1.tolist()),\n",
        "        \"Distinct-1\": distinct_1,\n",
        "        \"Distinct-2\": distinct_2\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# Save evaluation results\n",
        "df_results.to_csv(\"evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Evaluation complete! Results saved in 'evaluation_results.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740674392113
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-27 16:39:50.124879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740674390.138126    5920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740674390.142052    5920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-27 16:39:50.157470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "from collections import Counter\n",
        "from tqdm import tqdm  # Progress bar for better tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1740674396114
        }
      },
      "outputs": [],
      "source": [
        "# Load Hugging Face metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Load the dataset with responses and expected outputs\n",
        "df = pd.read_csv(\"final_combined_with_expected_responses.csv\")\n",
        "\n",
        "# Ensure responses are not empty or invalid\n",
        "df = df[df[\"output\"].notna()]\n",
        "\n",
        "# Load model and tokenizer for perplexity calculation\n",
        "perplexity_model_name = \"EleutherAI/gpt-neo-1.3B\"  # Efficient model\n",
        "perplexity_tokenizer = AutoTokenizer.from_pretrained(perplexity_model_name)\n",
        "perplexity_model = AutoModelForCausalLM.from_pretrained(perplexity_model_name).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1740674396169
        }
      },
      "outputs": [],
      "source": [
        "# Function to compute Perplexity (Batch Processing)\n",
        "def compute_perplexity_batch(texts):\n",
        "    encoded_inputs = perplexity_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = perplexity_model(**encoded_inputs, labels=encoded_inputs[\"input_ids\"])\n",
        "        losses = outputs.loss\n",
        "    return torch.exp(losses).cpu().numpy()  # Return as numpy array for efficiency\n",
        "\n",
        "# Compute DISTINCT-1 & DISTINCT-2 (Measures response diversity)\n",
        "def compute_distinct_ngram(texts, n):\n",
        "    ngram_counts = Counter()\n",
        "    total_ngrams = 0\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        if len(words) < n:  # Skip very short responses\n",
        "            continue\n",
        "        ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "        ngram_counts.update(ngrams)\n",
        "        total_ngrams += len(ngrams)\n",
        "    \n",
        "    return len(ngram_counts) / total_ngrams if total_ngrams > 0 else 0\n",
        "\n",
        "# Compute BLEU score (with Smoothing)\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    ref_tokens = reference.split()\n",
        "    hyp_tokens = hypothesis.split()\n",
        "    if len(hyp_tokens) == 0:  # Prevent errors on empty responses\n",
        "        return 0.0\n",
        "    return sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "# Define models to evaluate from the CSV file\n",
        "model_versions = [\n",
        "    (\"response_with_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_Traditional_responses\"),\n",
        "    (\"response_without_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_without_think\", \"generated_Fine_Tune_DeepMental_responses\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740674396811
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize evaluation storage\n",
        "evaluation_results = []\n",
        "\n",
        "# Run evaluation for each model\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Evaluating: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    \n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "\n",
        "    # Compute Perplexity (Batch Processing)\n",
        "    batch_size = 32  # Process 32 responses at a time\n",
        "    perplexities = []\n",
        "    for i in tqdm(range(0, len(predictions), batch_size), desc=f\"Perplexity for {model_name}\"):\n",
        "        batch = predictions[i:i+batch_size]\n",
        "        perplexities.extend(compute_perplexity_batch(batch))\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "    # Compute BLEU scores\n",
        "    bleu_scores = [compute_bleu(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "\n",
        "    # Compute BERTScore (Precision, Recall, F1)\n",
        "    P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
        "\n",
        "    # Compute DISTINCT scores\n",
        "    distinct_1 = compute_distinct_ngram(predictions, 1)\n",
        "    distinct_2 = compute_distinct_ngram(predictions, 2)\n",
        "\n",
        "    # Store results\n",
        "    evaluation_results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Response Type\": response_column,\n",
        "        \"Perplexity (Avg)\": np.mean(perplexities),\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"],\n",
        "        \"ROUGE-2\": rouge_scores[\"rouge2\"],\n",
        "        \"ROUGE-L\": rouge_scores[\"rougeL\"],\n",
        "        \"BLEU\": np.mean(bleu_scores),\n",
        "        \"BERTScore Precision\": np.mean(P.tolist()),\n",
        "        \"BERTScore Recall\": np.mean(R.tolist()),\n",
        "        \"BERTScore F1\": np.mean(F1.tolist()),\n",
        "        \"Distinct-1\": distinct_1,\n",
        "        \"Distinct-2\": distinct_2\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(evaluation_results)\n",
        "\n",
        "# Save evaluation results\n",
        "df_results.to_csv(\"evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Evaluation complete! Results saved in 'evaluation_results.csv'\")\n",
        "\n",
        "# Display results\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Evaluation Results\", dataframe=df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740676007583
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-27 17:06:34.325012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740675994.338655    7578 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740675994.342544    7578 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-27 17:06:34.357959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "from collections import Counter\n",
        "from tqdm import tqdm  # Progress bar for better tracking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1740676019682
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.151 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "      (1): LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "      (2-15): 14 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Hugging Face metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Load the dataset with responses and expected outputs\n",
        "df = pd.read_csv(\"final_combined_with_expected_responses.csv\")\n",
        "\n",
        "# Ensure responses are not empty or invalid\n",
        "df = df[df[\"output\"].notna()]\n",
        "\n",
        "# Load Llama 1B model from Unsloth\n",
        "model_name = \"unsloth/Llama-3.2-1B\"\n",
        "max_seq_length = 10000\n",
        "dtype = None\n",
        "load_in_4bit = True  # Efficient quantization\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable faster inference with Unsloth\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1740676099943
        }
      },
      "outputs": [],
      "source": [
        "# Define models to evaluate from the CSV file\n",
        "model_versions = [\n",
        "    (\"response_with_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_Traditional_responses\"),\n",
        "    (\"response_without_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_without_think\", \"generated_Fine_Tune_DeepMental_responses\"),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740676152647
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Perplexity for generated_Few_Shot_DeepMental_responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:08<00:00,  4.47it/s]\n",
            "Perplexity for generated_Fine_Tune_DeepMental_responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:12<00:00,  2.98it/s]\n",
            "Perplexity for generated_Fine_Tune_Traditional_responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:07<00:00,  5.10it/s]\n",
            "Perplexity for generated_Few_Shot_DeepMental_responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:07<00:00,  5.04it/s]\n",
            "Perplexity for generated_Fine_Tune_DeepMental_responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:12<00:00,  2.79it/s]\n"
          ]
        }
      ],
      "source": [
        "# --- Perplexity Computation ---\n",
        "def compute_perplexity_batch(texts):\n",
        "    if not isinstance(texts, list) or len(texts) == 0:\n",
        "        return []\n",
        "    texts = [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
        "    encoded_inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_inputs, labels=encoded_inputs[\"input_ids\"])\n",
        "        losses = outputs.loss\n",
        "    if losses.numel() == 1:\n",
        "        return [torch.exp(losses).item()] * len(texts)\n",
        "    else:\n",
        "        return torch.exp(losses).cpu().tolist()\n",
        "\n",
        "perplexity_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    batch_size = 32\n",
        "    perplexities = []\n",
        "    for i in tqdm(range(0, len(predictions), batch_size), desc=f\"Perplexity for {model_name}\"):\n",
        "        batch = predictions[i:i+batch_size]\n",
        "        perplexities.extend(compute_perplexity_batch(batch))\n",
        "    perplexity_results.append({\"Model\": model_name, \"Response Type\": response_column, \"Perplexity (Avg)\": np.mean(perplexities)})\n",
        "\n",
        "df_perplexity = pd.DataFrame(perplexity_results)\n",
        "df_perplexity.to_csv(\"perplexity_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Compute Perplexity\n",
        "perplexity_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "\n",
        "    if df_model.empty:\n",
        "        print(f\"Skipping {model_name} - No data found.\")\n",
        "        continue\n",
        "\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    batch_size = 32\n",
        "    perplexities = []\n",
        "\n",
        "    for i in tqdm(range(0, len(predictions), batch_size), desc=f\"Perplexity for {model_name}\"):\n",
        "        batch = predictions[i:i+batch_size]\n",
        "        perplexities.extend(compute_perplexity_batch(batch))\n",
        "\n",
        "    avg_perplexity = np.mean(perplexities) if perplexities else float(\"inf\")\n",
        "    perplexity_results.append({\"Model\": model_name, \"Response Type\": response_column, \"Perplexity (Avg)\": avg_perplexity})\n",
        "\n",
        "# Save Results\n",
        "df_perplexity = pd.DataFrame(perplexity_results)\n",
        "df_perplexity.to_csv(\"perplexity_results.csv\", index=False)\n",
        "\n",
        "print(\"Perplexity computation completed and saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1740676308580
        }
      },
      "outputs": [],
      "source": [
        "# --- ROUGE Computation ---\n",
        "rouge_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "    rouge_results.append({\n",
        "        \"Model\": model_name, \"Response Type\": response_column,\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"], \"ROUGE-2\": rouge_scores[\"rouge2\"], \"ROUGE-L\": rouge_scores[\"rougeL\"]\n",
        "    })\n",
        "\n",
        "df_rouge = pd.DataFrame(rouge_results)\n",
        "df_rouge.to_csv(\"rouge_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1740676323601
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- BLEU Computation ---\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    reference = str(reference) if isinstance(reference, str) else \"\"\n",
        "    hypothesis = str(hypothesis) if isinstance(hypothesis, str) else \"\"\n",
        "    ref_tokens = reference.split()\n",
        "    hyp_tokens = hypothesis.split()\n",
        "    if len(hyp_tokens) == 0:\n",
        "        return 0.0\n",
        "    return sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "bleu_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "    bleu_scores = [compute_bleu(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "    bleu_results.append({\"Model\": model_name, \"Response Type\": response_column, \"BLEU\": np.mean(bleu_scores)})\n",
        "\n",
        "df_bleu = pd.DataFrame(bleu_results)\n",
        "df_bleu.to_csv(\"bleu_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1740676711232
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- BERTScore Computation ---\n",
        "bert_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "    predictions = [str(p) if isinstance(p, str) else \"\" for p in predictions]\n",
        "    references = [str(r) if isinstance(r, str) else \"\" for r in references]\n",
        "    P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
        "    bert_results.append({\n",
        "        \"Model\": model_name, \"Response Type\": response_column,\n",
        "        \"BERTScore Precision\": np.mean(P.tolist()), \"BERTScore Recall\": np.mean(R.tolist()), \"BERTScore F1\": np.mean(F1.tolist())\n",
        "    })\n",
        "\n",
        "df_bert = pd.DataFrame(bert_results)\n",
        "df_bert.to_csv(\"bert_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1740676781392
        }
      },
      "outputs": [],
      "source": [
        "# --- DISTINCT Computation ---\n",
        "def compute_distinct_ngram(texts, n):\n",
        "    ngram_counts = Counter()\n",
        "    total_ngrams = 0\n",
        "    for text in texts:\n",
        "        words = str(text).split()\n",
        "        if len(words) < n:\n",
        "            continue\n",
        "        ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "        ngram_counts.update(ngrams)\n",
        "        total_ngrams += len(ngrams)\n",
        "    return len(ngram_counts) / total_ngrams if total_ngrams > 0 else 0\n",
        "\n",
        "distinct_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    distinct_1 = compute_distinct_ngram(predictions, 1)\n",
        "    distinct_2 = compute_distinct_ngram(predictions, 2)\n",
        "    distinct_results.append({\"Model\": model_name, \"Response Type\": response_column, \"Distinct-1\": distinct_1, \"Distinct-2\": distinct_2})\n",
        "\n",
        "df_distinct = pd.DataFrame(distinct_results)\n",
        "df_distinct.to_csv(\"distinct_results.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1740676785710
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Combine all results ---\n",
        "df_final = df_perplexity.merge(df_rouge, on=[\"Model\", \"Response Type\"]).merge(df_bleu, on=[\"Model\", \"Response Type\"]).merge(df_bert, on=[\"Model\", \"Response Type\"]).merge(df_distinct, on=[\"Model\", \"Response Type\"])\n",
        "df_final.to_csv(\"final_evaluation_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740677153234
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from tqdm import tqdm  # Progress bar for better tracking\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "from torchmetrics.text import Perplexity\n",
        "\n",
        "\n",
        "# Load Hugging Face evaluation metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "perplexity_metric = Perplexity()\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"final_combined_with_expected_responses.csv\")\n",
        "\n",
        "# Ensure responses are valid\n",
        "df = df[df[\"output\"].notna()]\n",
        "\n",
        "# Define models to evaluate from the CSV file\n",
        "model_versions = [\n",
        "    (\"response_with_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_DeepMental_responses\"),\n",
        "    (\"response_with_think\", \"generated_Fine_Tune_Traditional_responses\"),\n",
        "    (\"response_without_think\", \"generated_Few_Shot_DeepMental_responses\"),\n",
        "    (\"response_without_think\", \"generated_Fine_Tune_DeepMental_responses\")\n",
        "]\n",
        "\n",
        "# Step 1: Compute Perplexity using TorchMetrics\n",
        "perplexity_results = []\n",
        "batch_size = 32\n",
        "\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Computing Perplexity: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "\n",
        "    perplexities = []\n",
        "    for i in tqdm(range(0, len(predictions), batch_size), desc=f\"Perplexity for {model_name}\"):\n",
        "        batch = predictions[i:i+batch_size]\n",
        "        perplexities.extend(perplexity_metric(batch))  # Using torchmetrics\n",
        "\n",
        "    perplexity_results.append({\"Model\": model_name, \"Response Type\": response_column, \"Perplexity (Avg)\": np.mean(perplexities)})\n",
        "\n",
        "df_perplexity = pd.DataFrame(perplexity_results)\n",
        "df_perplexity.to_csv(\"evaluation_perplexity.csv\", index=False)\n",
        "print(\"âœ… Perplexity results saved in 'evaluation_perplexity.csv'\")\n",
        "\n",
        "# Step 2: Compute ROUGE using Hugging Face `evaluate`\n",
        "rouge_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Computing ROUGE: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "    \n",
        "    rouge_results.append({\n",
        "        \"Model\": model_name, \"Response Type\": response_column,\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"], \"ROUGE-2\": rouge_scores[\"rouge2\"], \"ROUGE-L\": rouge_scores[\"rougeL\"]\n",
        "    })\n",
        "\n",
        "df_rouge = pd.DataFrame(rouge_results)\n",
        "df_rouge.to_csv(\"evaluation_rouge.csv\", index=False)\n",
        "print(\"âœ… ROUGE results saved in 'evaluation_rouge.csv'\")\n",
        "\n",
        "# Step 3: Compute BLEU using Hugging Face `evaluate`\n",
        "bleu_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Computing BLEU: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].tolist()\n",
        "    references = df_model[\"output\"].tolist()\n",
        "\n",
        "    bleu_scores = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "    \n",
        "    bleu_results.append({\"Model\": model_name, \"Response Type\": response_column, \"BLEU\": bleu_scores[\"bleu\"]})\n",
        "\n",
        "df_bleu = pd.DataFrame(bleu_results)\n",
        "df_bleu.to_csv(\"evaluation_bleu.csv\", index=False)\n",
        "print(\"âœ… BLEU results saved in 'evaluation_bleu.csv'\")\n",
        "\n",
        "# Step 4: Compute BERTScore using `bert_score`\n",
        "bert_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Computing BERTScore: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].astype(str).tolist()\n",
        "    references = df_model[\"output\"].astype(str).tolist()\n",
        "\n",
        "    P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
        "\n",
        "    bert_results.append({\n",
        "        \"Model\": model_name, \"Response Type\": response_column,\n",
        "        \"BERTScore Precision\": np.mean(P.tolist()), \"BERTScore Recall\": np.mean(R.tolist()), \"BERTScore F1\": np.mean(F1.tolist())\n",
        "    })\n",
        "\n",
        "df_bert = pd.DataFrame(bert_results)\n",
        "df_bert.to_csv(\"evaluation_bert.csv\", index=False)\n",
        "print(\"âœ… BERTScore results saved in 'evaluation_bert.csv'\")\n",
        "\n",
        "# Step 5: Compute DISTINCT using Hugging Face `evaluate`\n",
        "distinct_results = []\n",
        "for response_column, model_name in model_versions:\n",
        "    print(f\"Computing DISTINCT: {model_name} ({response_column})...\")\n",
        "\n",
        "    df_model = df[df[\"model\"] == model_name]\n",
        "    predictions = df_model[response_column].astype(str).tolist()\n",
        "\n",
        "    distinct_scores = distinct.compute(predictions=predictions)\n",
        "    \n",
        "    distinct_results.append({\"Model\": model_name, \"Response Type\": response_column, \"Distinct-1\": distinct_scores[\"distinct1\"], \"Distinct-2\": distinct_scores[\"distinct2\"]})\n",
        "\n",
        "df_distinct = pd.DataFrame(distinct_results)\n",
        "df_distinct.to_csv(\"evaluation_distinct.csv\", index=False)\n",
        "print(\"âœ… DISTINCT results saved in 'evaluation_distinct.csv'\")\n",
        "\n",
        "# Step 6: Combine all results\n",
        "df_final = df_perplexity.merge(df_rouge, on=[\"Model\", \"Response Type\"], how=\"left\")\n",
        "df_final = df_final.merge(df_bleu, on=[\"Model\", \"Response Type\"], how=\"left\")\n",
        "df_final = df_final.merge(df_bert, on=[\"Model\", \"Response Type\"], how=\"left\")\n",
        "df_final = df_final.merge(df_distinct, on=[\"Model\", \"Response Type\"], how=\"left\")\n",
        "\n",
        "df_final.to_csv(\"evaluation_results.csv\", index=False)\n",
        "print(\"âœ… Final evaluation results saved in 'evaluation_results.csv'\")\n",
        "\n",
        "# Display final results\n",
        "tools.display_dataframe_to_user(name=\"Final Evaluation Results\", dataframe=df_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
